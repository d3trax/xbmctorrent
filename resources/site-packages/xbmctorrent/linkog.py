
import string
from xbmctorrent import plugin

POOL_WORKERS = 4

class linko_unexpected_url(Exception):

    def __int__(self, arg):
        self.args = arg

class cookies:
    def __init__(self, cookies):
        self.cookieslist = {}
        cookies = str(cookies)
        cookiesPar = string.split(cookies, ";")
        for cookie in cookiesPar:
            data = cookie.split('=')
            if len(data) > 1:
                k = data[0]
                v = data[1]
                self.cookieslist[str(k).strip()] = v

    def get(self, key):
        if key in self.cookieslist:
            return self.cookieslist[key]
        else:
            return None

    def set(self, key, val):
        if key is None:
            return
        self.cookieslist[key] = val

    def getList(self):
        return self.cookieslist.items()

class linko_session:

    def __init__(self):
        self.cookieslist = {}

    def get(self, key):
        if key in self.cookieslist:
            return self.cookieslist[key]
        return None

    def set(self, key, val):
        if key is None:
            return
        self.cookieslist[key] = val

    def isAuth(self):
        return self.get("login") is not None

    def getSession(self):
        return self.cookieslist

class linko_grabber:

    def __init__(self, header={}, base_url="", logins=None):
        self.base_url=base_url
        self.header=header
        self.cookies=None
        if logins is None:
            logins = {'user':'', 'pass':''}
        self.user=logins
        self.reauth()

    def _get_url(self, url, params={}, expect_url='', headers={}, session={}, parse_data=False):
        import urllib2
        from urlparse import urlparse
        from contextlib import closing

        if params:
            import urllib
            req = urllib2.Request(url, urllib.urlencode(params))
        else:
            req = urllib2.Request(url)

        for k, v in headers.items():
            req.add_header(k, v)

        cookes = ""
        if self.cookies is not None:
            for k, v in self.cookies.getList():
                if v is not None:
                    cookes += k+"="+v+"; "

        if session is not None:
            for k, v in session.items():
                if v is not None:
                    cookes += k+"="+v+"; "

        if len(cookes):
            req.add_header("Cookie", cookes)

        try:
            with closing(urllib2.urlopen(req)) as response:
                if expect_url:
                    if response.url is None:
                        raise linko_unexpected_url("Got unexpected \"%s\" " % response.url)
                    u = urlparse(response.url)
                    eu = urlparse(expect_url)
                    if eu.netloc + eu.path != u.netloc + u.path:
                        raise linko_unexpected_url("Got \"%s\", but expected \"%s\" " % (u.netloc + u.path, eu.netloc + eu.path))

                if response.msg != 'OK':
                    return None

                c = response.headers.get("Set-Cookie", "")
                if len(c):
                    c = cookies(c)
                    if self.cookies is None:
                        self.cookies = c
                    else:
                        for k, v in c.getList():
                           self.cookies.set(k, v)

                if parse_data:
                    data = response.read()
                    if response.headers.get("Content-Encoding", "") == "gzip":
                        import zlib
                        data = zlib.decompressobj(16 + zlib.MAX_WBITS).decompress(data)
                    return data
                else:
                    return None
        except urllib2.HTTPError, linko_session_ex:
            return None

    def auth(self):
        index_url = self.base_url+"/index.php"
        self._get_url(index_url, headers=self.header, expect_url=self.base_url+"/login.php")
        login_url = self.base_url+"/takelogin.php"
        self._get_url(login_url, {
                'username':self.user['user'],
                'password':self.user['pass'],
                'commit':'Prisijungti',
                'login_cookie':1
            }, headers=self.header)
        session = linko_session()
        session.set('login', self.cookies.get('login'))
        session.set('PHPSESSID', self.cookies.get('PHPSESSID'))

        if self.cookies.get('login') is None:
            return None

        return session

    def reauth(self, force=False):
        from xbmctorrent.caching import shelf
        with shelf("com.linko.session") as session:
            if not session or force:
                sess = self.auth()
                if sess and sess.isAuth():
                    session.update(sess.getSession() or None)
        self.session = session

    def categories(self):
            list = [
                {'name':'Movies', 'value':'29'},
                {'name':'Movies HD', 'value':'52'},
                {'name':'Movies LT', 'value':'53'},
                {'name':'Movies LT HD', 'value':'61'},
                {'name':'Movies RU', 'value':'51'},
                {'name':'Movies RU HD', 'value':'64'},
                {'name':'TV', 'value':'30'},
                {'name':'TV HD', 'value':'60'},
                {'name':'TV LT', 'value':'28'},
                {'name':'TV LT HD', 'value':'62'},
                {'name':'TV RU', 'value':'65'},
                {'name':'Music Videos', 'value':'31'},
                {'name':'DVD', 'value':'25'},
                {'name':'DVD LT', 'value':'26'},
            ]
            return list

    def search(self, cat, text, page, xbmc=None, dialog=None):
        import urllib
        from bs4 import BeautifulSoup
        from xbmctorrent.caching import shelf
        from urllib import unquote
        from xbmctorrent.magnet import display_name
        from urlparse import urlparse, parse_qs

        with shelf("com.linko.search.%s.%s.%s" % (cat, text, page)) as data:
            if not data:
                url = self.base_url+("/browse.php?%s" % urllib.urlencode({
                    'c'+cat:1,
                    "search":urllib.unquote(text),
                    'incldead':0,
                    "page":page,
                }, True))

                plugin.log.info("Got searching: %s" % url)

                if dialog:
                    dialog.update(line2="Downloading data", percent=0)
                content = self._get_url(url, headers=self.header, parse_data=True, expect_url=url)
                if dialog:
                    dialog.update(line2="Parsing data", percent=100)
                soup = BeautifulSoup(content, "html5lib")
                nodes = soup.findAll("a", "index")
                i = 0
                for node in nodes:
                    link = str(node.attrs['href'])
                    title = None
                    is_new = False
                    for bitem in node.parent.find_all("b"):
                        if bitem.parent.name == 'a':
                            title = bitem.text
                        else:
                            is_new = True
                    if dialog:
                        dialog.update(line2="Parsing data - " + title, percent=int((i * 100.0 / len(nodes))) )
                    description_short = node.parent.find("span").text

                    ourl = urlparse(link)
                    q = parse_qs(ourl.query)
                    if q is None or q['id'] is None:
                        continue
                    id = q['id'][0]

                    if link is not None:
                        i = i+1
                        uri = unquote(self.base_url +"/"+ link)
                        magnet_uri = self.ensure_magnet(uri)
                        if magnet_uri is None:
                            continue
                        filename = display_name(magnet_uri)
                        data.update({i: {"id":id, "magnet_uri" : magnet_uri, "filename" : filename, "name": title, "url": uri, "short_desc": description_short, "is_new": is_new}} or {})
        list = []
        for r in data:
            list.append(data[r])

        if dialog:
            dialog.update(line2="Processing...", percent=90)
        return list

    def downloadDetails(self,id):
        from xbmctorrent.caching import shelf
        with shelf("com.linko.details_raw.%s" % id) as data:
            if not data:
                url = self.base_url+("/details?%s" % id)
                content = self._get_url(url, headers=self.header, parse_data=True, expect_url=url)
                if content:
                    data.update({'content':content})
        return data

    def getDetaisl(self, id):
        from bs4 import BeautifulSoup
        from xbmctorrent.caching import shelf

        with shelf("com.linko.details.%s" % id) as data:
            if not data:
                content = self.downloadDetails(id)
                if content is None or not ('content' in content):
                    return None
                content = content['content']
                soup = BeautifulSoup(content, "html5lib")
                nodes = soup.findAll("td", "descr_text")
                if len(nodes) < 1:
                    return None
                text = ""
                p_tags = nodes[0].findAll(text=True)
                for i, p_tag in enumerate(p_tags):
                    if i < 3 or len(p_tag) < 1:
                        continue
                    text = text + p_tag + "\n"

                images = nodes[0].findAll("img")
                imagesList = []
                for img in images:
                    imagesList.append(str(img.attrs['src']))
                data.update({'images':imagesList, 'text':text})
        return data

    def download(self, url):
        from xbmctorrent.caching import closing
        from xbmctorrent.caching import cache_path
        import uuid

        name = unicode(uuid.uuid4())
        path = "com.linko.torrents.%s" % name
        with closing(open(path, 'w')) as d:
            content = self._get_url(url, headers=self.header, parse_data=True, expect_url=url)
            if content is None:
                d.close()
                return None
            d.write(content)
            d.flush()
            d.close()
        return cache_path(path)

    def from_torrent_url(self, url):
        import base64
        import bencode
        import hashlib
        import urllib
        torrent_data = self._get_url(url, parse_data=True, expect_url=url)
        if not torrent_data or torrent_data is None:
            return None
        metadata = bencode.bdecode(torrent_data)
        hashcontents = bencode.bencode(metadata['info'])
        digest = hashlib.sha1(hashcontents).digest()
        b32hash = base64.b32encode(digest)
        params = {
            'dn': metadata['info']['name'],
            'tr': metadata['announce'],
        }
        paramstr = urllib.urlencode(params)
        return 'magnet:?%s&%s' % ('xt=urn:btih:%s' % b32hash, paramstr)

    def ensure_magnet(self, uri):
        if not uri.startswith("magnet:"):
            uri = self.from_torrent_url(uri)
        return uri

def find_image(item):
    if item.get("description"):
        import re
        image_re = r"(https?://(?:[\w\-]+\.)+[a-z]{2,6}(?:/[^/#?\s]+)+\.(?:jpg|gif|png))"
        result = re.search(image_re, item["description"])
        if result:
            item["img"] = result.group(1)

def check_imdb_id(item):
    if item.get("description"):
        import re
        match = re.search(r"(tt\d+)", item["description"])
        if match:
            item["imdb_id"] = match.group(1)

import xml.etree.ElementTree as ET
import re

class MyXMLParser(ET.XMLParser):

    rx = re.compile("&#([0-9]+);|&#x([0-9a-fA-F]+);")

    def feed(self,data):
        m = self.rx.search(data)
        if m is not None:
            target = m.group(1)
            if target:
                num = int(target)
            else:
                num = int(m.group(2), 16)
            if not(num in (0x9, 0xA, 0xD) or 0x20 <= num <= 0xD7FF
                   or 0xE000 <= num <= 0xFFFD or 0x10000 <= num <= 0x10FFFF):
                # is invalid xml character, cut it out of the stream
                print 'removing %s' % m.group()
                mstart, mend = m.span()
                mydata = data[:mstart] + data[mend:]
        else:
            mydata = data
        super(MyXMLParser,self).feed(mydata)



def parse(data, url):
    import xbmc
    from itertools import izip_longest
    from concurrent import futures
    from contextlib import nested, closing
    from xbmctorrent.utils import SafeDialogProgress, get_quality_from_name, get_show_info_from_name, normalize_release_tags
    from xbmctorrent import tmdb
    import re
    # from xbmctorrent.library import library_context
    #import lxml.etree
    # import codecs

    try:
        #parser = lxml.etree.XMLParser(encoding='utf-8', recover=True)
        # root = ET.fromstring(data, parser=parser)
        data = re.sub(r'[^\x00-\x7f]',r' ',data)
        root = ET.fromstring(data)
    except Exception as e:
        plugin.log.error("Exception: '%s' on '%s'" % (e.message, url))
        parser = MyXMLParser(encoding='utf-8')
        root = ET.fromstring(data, parser=parser)

    def _text(node, path):
        n = node.find(path)
        if n is not None:
            return n.text
    def _attr(node, path, attrib):
        n = node.find(path)
        if n is not None:
            return n.attrib.get(attrib)

    items = []
    for node in root.getiterator("item"):
        item = {
            "title": _text(node, "title"),
            "description": _text(node, "description"),
            "category": _text(node, "category") or "Unknown",
            "pub_date": _text(node, "pubDate"),
            "href": _text(node, "link"),
        }
        find_image(item)
        check_imdb_id(item)
        items.append(item)

    tmdb_list = []

    with closing(SafeDialogProgress(delay_close=0)) as dialog:
        dialog.create(plugin.name)
        dialog.update(percent=0, line1="Fetching torrent information...", line2="", line3="")

        with futures.ThreadPoolExecutor(max_workers=POOL_WORKERS) as pool:
            futures = []
            for item in items:
                if item.get("imdb_id"):
                    futures.append(pool.submit(tmdb.get, item["imdb_id"]))
                else:
                    futures.append(None)
            state = {"done": 0}
            def on_item(future):
                state["done"] += 1
                dialog.update(
                    percent=int(state["done"] * 100.0 / len(filter(None, futures))),
                )
            [future.add_done_callback(on_item) for future in futures if future]
            while not all(future.done() for future in futures if future):
                if dialog.iscanceled():
                    return
                xbmc.sleep(100)
    tmdb_list = [future and future.result() or None for future in futures]

    for item, tmdb_data in izip_longest(items, tmdb_list):
        if tmdb_data:
            list_item = tmdb.get_list_item(tmdb_data)
            release_tags = normalize_release_tags(item["title"], list_item["label"])
            if release_tags:
                list_item["label"] = "%s (%s)" % (list_item["label"], release_tags)
        else:
            list_item = {
                "label": item["title"],
                "icon": item.get("img") or "",
                "thumbnail": item.get("img") or "",
                "info": {
                    "genre": item["category"],
                }
            }
        list_item.update({
            "path": plugin.url_for("play", uri=item["href"]),
            "is_playable": True,
        })
        list_item.setdefault("info", {}).update({
            "genre": "%s " % (list_item.get("info", {}).get("genre") or ""),
        })
        list_item.setdefault("stream_info", {}).update(get_quality_from_name(item["title"]))

        yield list_item